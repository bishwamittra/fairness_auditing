---
layout: about
title: Home
permalink: /
subtitle: 

# profile:
#   align: right
#   image: prof_pic.jpg
#   image_circular: false # crops the image to make it circular
#   address: >
#     <p>555 your office number</p>
#     <p>123 your address street</p>
#     <p>Your City, State 12345</p>

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page


---

# <b><b>Auditing Bias of Machine Learning Algorithms: Tools and Overview</b></b>

### <b>Tutorial at <a href='https://ijcai-23.org/'>IJCAI 2023</a>, Macao</b>

Any publicly used technology presently undergoes an audit mechanism, where we aim to understand the impacts and limitations of using that technology, and why they are caused. As Machine Learning (ML) is becoming the pervasive technology of our time and the discourse on bias induced by ML systems is attracting attention, it is a genuine concern to understand how to audit an ML system. 

In this tutorial, we discuss algorithmic developments and existing software to 

1. How to choose a compatible fairness metric to quantify bias encountered in an application?
2. How to quantify the bias encountered in the predictions of ML systems accurately and sample-efficiently?
3. How to explain different sources yielding the bias up to different levels of granularity?

We conclude the conundrum or open it for the audience through a case-study of auditing an ML classifier.

## Resources

<table>
<tr>
<td>Part 1: Choose your fairness metric</td>
<td> <a href=" ">slides</a> </td> <td> <a href=" ">video</a> </td></tr>
<tr>
<td>Part 2: Estimate your fairness metric</td>
<td> <a href=" ">slides</a> </td> <td> <a href=" ">video</a> </td></tr>
<tr>
<td>Part 3: Attribute the fairness metric to features/input</td>
<td> <a href=" ">slides</a> </td> <td> <a href=" ">video</a> </td></tr>
<tr>
<td>Part 4:  A case study and open problems</td>
<td> <a href=" ">slides</a> </td> <td> <a href=" ">video</a> </td>
</tr>
</table>

A summary of the above material is compiled as a note <a href=" ">here</a>.

## Speakers

<div class="container" style="margin-bottom:1cm;">
  <div class="row">
<div class="col-sm-6 col-md-3">
<img alt="image" width="140px" src="assets/img/bg.jpg" class="img-responsive" align="center">
</div>
<div class="col-sm-6 col-md-9">
<a href='https://bishwamittra.github.io'>Bishwamittra Ghosh</a> is a PhD candidate in School of Computing, National University of Singapore. His research interest is on fairness and interpretability in machine learning. He applies automated reasoning, formal methods, and statistics to improve fairness and interpretability in machine learning. His research has thrived through multiple collaborations and internships in industry and academia. 
</div>
  </div>
</div>

<div class="container"  style="margin-bottom:1cm;">
  <div class="row">
<div class="col-sm-6 col-md-3">
<img alt="image" width="140px" src="assets/img/db.jpg" class="img-responsive" align="center">
</div>
    <div class="col-sm-6 col-md-9">
<a href='https://debabrota-basu.github.io'>Debabrota Basu</a> is a researcher (equivalent of Assoc. Prof.) in the Scool team (previously Sequel) of Inria at Universit√© de Lille and CNRS, France. His research interest is developing algorithms and analysis to build theoretically-grounded responsible AI systems. Specially, he studies how to develop robust, private, fair, and explainable algorithms for online learning, bandits, and reinforcement learning problems. He is also interested in developing the statistical theory behind sample-efficiently and consitently auditing ML systems.
    </div>
  </div>
</div>

